{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Create a new database in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#set this user line \n",
    "user = 'jared'\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"entity_phone\"\n",
    "os.environ['SNORKELDBNAME'] = \"location_extraction\"\n",
    "\n",
    "if user == 'accenture':\n",
    "    os.environ['SNORKELDB'] = 'postgresql://localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "    sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/')\n",
    "elif user == 'jared':\n",
    "    os.environ['SNORKELDB'] = 'postgres://jdunnmon:123@localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "    sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/')\n",
    "    \n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine('postgresql://localhost:5432/', isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Defining a Candidate Schema2) Candidate Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "\n",
    "Location_Extraction = candidate_subclass('location_extraction', [\"location\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "### Configuring an `HTMLPreprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "\n",
    "if user == 'accenture':\n",
    "    docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/data/profiles_chtap/'\n",
    "elif user == 'jared':\n",
    "    docs_path = '/lfs/local/0/jdunnmon/chtap/data/s3/chtap_profiles_20170928/'\n",
    "\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `OmniParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 45.3 s, sys: 1.16 s, total: 46.4 s\n",
      "Wall time: 44min 11s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 1019\n",
      "Phrases: 239581\n",
      "Table 1571\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase,Table\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Phrases:\", session.query(Phrase).count()\n",
    "print \"Table\", session.query(Table).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dividing the Corpus into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 816\n",
      "dev: 102\n",
      "test: 101\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "#pprint([x.name for x in train_docs])\n",
    "print \"train:\",len(train_docs)\n",
    "print \"dev:\" ,len(dev_docs)\n",
    "print \"test:\",len(test_docs)\n",
    "# from pprint import pprint\n",
    "# pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Candidate Extraction & Multimodal Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=True) \n",
    "\n",
    "####Define a relation's ContextSpaces\n",
    "\n",
    "from snorkel.contrib.fonduer.fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=6, split_tokens=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate Throttlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    #print len(cand_right_tokens)\n",
    "    #print cand_right_tokens#(get_right_ngrams(location,window=4))\n",
    "    for cand in cand_right_tokens:\n",
    "        #print \"[\"+cand+\"]\"\n",
    "        if cand not in list_currencies:\n",
    "            #print \"[\"+cand+\"]\"\n",
    "            #print location\n",
    "            return location\n",
    "    \n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 284 ms, sys: 283 ms, total: 567 ms\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "\n",
    "#                         candidate_filter=candidate_filter\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 7121\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the candidate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.fonduer.lf_helpers import*\n",
    "from snorkel.contrib.fonduer.candidates import*\n",
    "###################### print candidates and text spans\n",
    "\n",
    "# for cand in train_cands:\n",
    "#     print cand\n",
    "#     print cand.get_parent()\n",
    "    #print cand\n",
    "#cand = train_cands[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"Houston Body Rubs\", sentence=392310, chars=[0,16], words=[0,2]))\n",
      "location_extraction(Span(\"Hawaii\", sentence=497348, chars=[28,33], words=[5,5]))\n",
      "location_extraction(Span(\"Hawaii\", sentence=497353, chars=[0,5], words=[0,0]))\n"
     ]
    }
   ],
   "source": [
    "cand_16= train_cands[16]\n",
    "print cand_16\n",
    "cand_18= train_cands[18]\n",
    "print cand_18\n",
    "cand_19= train_cands[19]\n",
    "print cand_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text for the 16th candidate:\n",
      "Phrase (Doc: a07e995c-1bae-4b13-9843-c13e34a788d4, Index: 0, Text: Houston Body Rubs in Texas)\n",
      "16th candidate\n",
      ": location_extraction(Span(\"Houston Body Rubs\", sentence=392310, chars=[0,16], words=[0,2]))\n",
      "ancestor of 16th candidate\n",
      ": ['html', 'head', 'title']\n",
      "***************************************************\n",
      "text for the 17th candidate:\n",
      "Phrase (Doc: a07e995c-1bae-4b13-9843-c13e34a788d4, Index: 25, Text: Texas    Â»)\n",
      "17th candidate: location_extraction(Span(\"Texas\", sentence=392410, chars=[0,4], words=[0,0]))\n",
      "ancestor of 17th candidate\n",
      ": ['html', 'body', 'div', 'div']\n",
      "***************************************************\n",
      "text for the 19th candidate:\n",
      "Phrase (Doc: 5f8b3b55-f796-42aa-a49a-e10e77fd8834, Index: 53, Text: Hawaii By Night)\n",
      "19th candidate: location_extraction(Span(\"Hawaii\", sentence=497353, chars=[0,5], words=[0,0]))\n",
      "ancestor of 19th candidate\n",
      ": ['html', 'body', 'div', 'div', 'div', 'div', 'li']\n"
     ]
    }
   ],
   "source": [
    "cand_16= train_cands[16]\n",
    "print \"text for the 16th candidate:\\n\", cand_16.get_parent()\n",
    "print \"16th candidate\\n:\",cand_16\n",
    "ance_16 = get_ancestor_tag_names(cand_16)\n",
    "print \"ancestor of 16th candidate\\n:\", ance_16 \n",
    "print \"***************************************************\"\n",
    "cand_17= train_cands[17]\n",
    "print \"text for the 17th candidate:\\n\", cand_17.get_parent()\n",
    "print \"17th candidate:\",cand_17\n",
    "ance_17 = get_ancestor_tag_names(cand_17)\n",
    "print \"ancestor of 17th candidate\\n:\", ance_17\n",
    "print \"***************************************************\"\n",
    "\n",
    "cand_19= train_cands[19]\n",
    "print \"text for the 19th candidate:\\n\", cand_19.get_parent()\n",
    "print \"19th candidate:\",cand_19\n",
    "ance_19 = get_ancestor_tag_names(cand_18)\n",
    "print \"ancestor of 19th candidate\\n:\", ance_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Repeating for development and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 918\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 878\n",
      "CPU times: user 48.6 s, sys: 2.5 s, total: 51.1 s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print \"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 7121\n",
      "['html', 'head', 'title']\n",
      "Phrase (Doc: d7e35a5a-9966-4b1c-a322-59d3c591e822, Index: 0, Text: Boston Escorts - Boston Female Escorts - Female Escorts in Boston - Massachusetts Call Girls)\n"
     ]
    }
   ],
   "source": [
    "dev_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()\n",
    "print \"Number of candidates:\", len(train_cands)\n",
    "dev_cand1= dev_cands[300]\n",
    "# for cand in dev_cand:\n",
    "#     print cand\n",
    "#     print cand.get_parent()\n",
    "print get_ancestor_tag_names(dev_cand1)\n",
    "print dev_cand1.get_parent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Getting Place Names from Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting google place autocomplete API\n",
    "import googlemaps as gm\n",
    "\n",
    "def get_candidate_locations(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    jsn: full json structure returned from API call\n",
    "    plcs: list of candidate location strings\n",
    "    \"\"\"\n",
    "    api_key = 'AIzaSyDbk3lLZHuQVKDRBN99_oz-p4AJjIzhA0w' \n",
    "    gmaps = gm.Client(key=api_key)\n",
    "    qo = gm.places.places_autocomplete(gmaps,test_loc)\n",
    "    cl = [a['description'] for a in qo]\n",
    "    return qo,cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOCATION:\n",
      "Jupiter\n",
      "\n",
      "CANDIDATE LOCATIONS:\n",
      "Jupiter, Shattuck Avenue, Berkeley, CA, United States\n",
      "Jupiter Research Foundation Inc., 2nd Street, Los Altos, CA, United States\n",
      "Jupiter, FL, United States\n",
      "Jupiter Systems, Huntwood Avenue, Hayward, CA, United States\n",
      "Jupiter Drive, Milpitas, CA, United States\n",
      "\n",
      "API RETURN STRUCTURE KEYS:\n",
      "terms\n",
      "description\n",
      "reference\n",
      "structured_formatting\n",
      "matched_substrings\n",
      "place_id\n",
      "id\n",
      "types\n"
     ]
    }
   ],
   "source": [
    "#testing on a single candidate\n",
    "test_loc = 'Jupiter'\n",
    "query_out,can_locs = get_candidate_locations(test_loc)\n",
    "\n",
    "print \"TEST LOCATION:\"\n",
    "print test_loc\n",
    "\n",
    "print \"\" \n",
    "\n",
    "print \"CANDIDATE LOCATIONS:\"\n",
    "for p in can_locs: print p\n",
    "    \n",
    "print \"\"\n",
    "    \n",
    "print \"API RETURN STRUCTURE KEYS:\"\n",
    "for p in query_out[0].keys(): print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
