{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Creat a new database in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "location_extraction\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"entity_phone\"\n",
    "#os.environ['SNORKELDB']\n",
    "os.environ['SNORKELDBNAME'] = \"location_extraction\"\n",
    "print os.environ['SNORKELDBNAME']\n",
    "os.environ['SNORKELDB'] = 'postgresql://localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine('postgresql://localhost:5432/', isolation_level=\"AUTOCOMMIT\")\n",
    "\n",
    "\n",
    "sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Defining a Candidate Schema2) Candidate Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "\n",
    "Location_Extraction = candidate_subclass('location_extraction', [\"location\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "### Configuring an `HTMLPreprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "\n",
    "docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/data/profiles_chtap/'\n",
    "\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `OmniParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 10.6 s, sys: 326 ms, total: 10.9 s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 364\n",
      "Phrases: 76473\n",
      "Table 891\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase,Table\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Phrases:\", session.query(Phrase).count()\n",
    "print \"Table\", session.query(Table).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dividing the Corpus into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 292\n",
      "dev: 36\n",
      "test: 36\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "#pprint([x.name for x in train_docs])\n",
    "print \"train:\",len(train_docs)\n",
    "print \"dev:\" ,len(dev_docs)\n",
    "print \"test:\",len(test_docs)\n",
    "# from pprint import pprint\n",
    "# pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Candidate Extraction & Multimodal Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=True) \n",
    "\n",
    "####Define a relation's ContextSpaces\n",
    "\n",
    "from snorkel.contrib.fonduer.fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=6, split_tokens=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate Throttlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    #print len(cand_right_tokens)\n",
    "    #print cand_right_tokens#(get_right_ngrams(location,window=4))\n",
    "    for cand in cand_right_tokens:\n",
    "        #print \"[\"+cand+\"]\"\n",
    "        if cand not in list_currencies:\n",
    "            #print \"[\"+cand+\"]\"\n",
    "            #print location\n",
    "            return location\n",
    "    \n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 87.3 ms, sys: 52.8 ms, total: 140 ms\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "\n",
    "#                         candidate_filter=candidate_filter\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 2574\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the candidate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.fonduer.lf_helpers import*\n",
    "from snorkel.contrib.fonduer.candidates import*\n",
    "###################### print candidates and text spans\n",
    "\n",
    "# for cand in train_cands:\n",
    "#     print cand\n",
    "#     print cand.get_parent()\n",
    "    #print cand\n",
    "#cand = train_cands[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"Dallas\", sentence=448768, chars=[0,5], words=[0,0]))\n",
      "location_extraction(Span(\"Dallas\", sentence=502951, chars=[13,18], words=[3,3]))\n",
      "location_extraction(Span(\"Dallas Fort Worth\", sentence=448824, chars=[0,16], words=[0,2]))\n"
     ]
    }
   ],
   "source": [
    "cand_16= train_cands[16]\n",
    "print cand_16\n",
    "\n",
    "cand_18= train_cands[18]\n",
    "print cand_18\n",
    "cand_19= train_cands[19]\n",
    "print cand_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text for the 16th candidate:\n",
      "Phrase (Doc: 909e33a5-618a-481a-b7ea-250560c01b6b, Index: 33, Text: Dallas, Texas Female Escort & GFE available for Incall and Outcall.)\n",
      "16th candidate\n",
      ": location_extraction(Span(\"Dallas\", sentence=448768, chars=[0,5], words=[0,0]))\n",
      "ancestor of 16th candidate\n",
      ": ['html', 'body', 'div', 'div', 'table', 'tr', 'td', 'a']\n",
      "***************************************************\n",
      "text for the 17th candidate:\n",
      "Phrase (Doc: c1ea3cbb-9a8a-4a18-8f17-e110409baf83, Index: 34, Text: New York City, New York Female Escort available for Incall.)\n",
      "17th candidate: location_extraction(Span(\"New York\", sentence=462318, chars=[15,22], words=[4,5]))\n",
      "ancestor of 17th candidate\n",
      ": ['html', 'body', 'div', 'div', 'table', 'tr', 'td', 'a']\n",
      "***************************************************\n",
      "text for the 19th candidate:\n",
      "Phrase (Doc: 909e33a5-618a-481a-b7ea-250560c01b6b, Index: 47, Text: Dallas Fort Worth plano)\n",
      "19th candidate: location_extraction(Span(\"Dallas Fort Worth\", sentence=448824, chars=[0,16], words=[0,2]))\n",
      "ancestor of 19th candidate\n",
      ": ['html', 'body', 'div', 'div', 'table', 'tr', 'td', 'a']\n"
     ]
    }
   ],
   "source": [
    "cand_16= train_cands[16]\n",
    "print \"text for the 16th candidate:\\n\", cand_16.get_parent()\n",
    "print \"16th candidate\\n:\",cand_16\n",
    "ance_16 = get_ancestor_tag_names(cand)\n",
    "print \"ancestor of 16th candidate\\n:\", ance_16 \n",
    "print \"***************************************************\"\n",
    "cand_17= train_cands[17]\n",
    "print \"text for the 17th candidate:\\n\", cand_17.get_parent()\n",
    "print \"17th candidate:\",cand_17\n",
    "ance_17 = get_ancestor_tag_names(cand)\n",
    "print \"ancestor of 17th candidate\\n:\", ance_17\n",
    "print \"***************************************************\"\n",
    "\n",
    "cand_19= train_cands[19]\n",
    "print \"text for the 19th candidate:\\n\", cand_19.get_parent()\n",
    "print \"19th candidate:\",cand_19\n",
    "ance_19 = get_ancestor_tag_names(cand)\n",
    "print \"ancestor of 19th candidate\\n:\", ance_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Repeating for development and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 320\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 313\n",
      "CPU times: user 11 s, sys: 866 ms, total: 11.8 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print \"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 2574\n",
      "['html', 'body', 'div', 'div']\n",
      "Phrase (Doc: d22b143e-1d97-4e84-a0d9-b5c36c553bc7, Index: 23, Text: United States Â»)\n"
     ]
    }
   ],
   "source": [
    "dev_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()\n",
    "print \"Number of candidates:\", len(train_cands)\n",
    "dev_cand1= dev_cands[300]\n",
    "# for cand in dev_cand:\n",
    "#     print cand\n",
    "#     print cand.get_parent()\n",
    "print get_ancestor_tag_names(dev_cand1)\n",
    "print dev_cand1.get_parent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Getting Place Names from Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting google place autocomplete API\n",
    "import googlemaps as gm\n",
    "\n",
    "def get_candidate_locations(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    jsn: full json structure returned from API call\n",
    "    plcs: list of candidate location strings\n",
    "    \"\"\"\n",
    "    api_key = 'AIzaSyDbk3lLZHuQVKDRBN99_oz-p4AJjIzhA0w' \n",
    "    gmaps = gm.Client(key=api_key)\n",
    "    qo = gm.places.places_autocomplete(gmaps,test_loc)\n",
    "    cl = [a['description'] for a in qo]\n",
    "    return qo,cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOCATION:\n",
      "Jupiter\n",
      "\n",
      "CANDIDATE LOCATIONS:\n",
      "Jupiter, Shattuck Avenue, Berkeley, CA, United States\n",
      "Jupiter Research Foundation Inc., 2nd Street, Los Altos, CA, United States\n",
      "Jupiter, FL, United States\n",
      "Jupiter Systems, Huntwood Avenue, Hayward, CA, United States\n",
      "Jupiter Drive, Milpitas, CA, United States\n",
      "\n",
      "API RETURN STRUCTURE KEYS:\n",
      "terms\n",
      "description\n",
      "reference\n",
      "structured_formatting\n",
      "matched_substrings\n",
      "place_id\n",
      "id\n",
      "types\n"
     ]
    }
   ],
   "source": [
    "#testing on a single candidate\n",
    "test_loc = 'Jupiter'\n",
    "query_out,can_locs = get_candidate_locations(test_loc)\n",
    "\n",
    "print \"TEST LOCATION:\"\n",
    "print test_loc\n",
    "\n",
    "print \"\" \n",
    "\n",
    "print \"CANDIDATE LOCATIONS:\"\n",
    "for p in can_locs: print p\n",
    "    \n",
    "print \"\"\n",
    "    \n",
    "print \"API RETURN STRUCTURE KEYS:\"\n",
    "for p in query_out[0].keys(): print p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
